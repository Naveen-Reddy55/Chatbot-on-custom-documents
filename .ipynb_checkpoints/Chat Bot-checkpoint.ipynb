{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "dd578162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain,\n",
    "    LLMChain\n",
    ")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "import keyboard\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bf7e2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vector_store(folder_path):\n",
    "    \n",
    "    openai_key=os.environ['OPENAI_API_KEY']\n",
    "    all_text=[]\n",
    "\n",
    "    for i in os.listdir(folder_path):\n",
    "        filename=os.path.join(str(folder_path),i)\n",
    "        if filename.endswith('.pdf'):\n",
    "            reader = PdfReader(filename)\n",
    "            page_texts = [page.extract_text() for page in reader.pages]\n",
    "            text = \" \".join(page_texts)\n",
    "            all_text.append(text)\n",
    "\n",
    "        elif filename.endswith('.txt'):\n",
    "            with open(filename, encoding=\"utf8\") as f:\n",
    "                all_text.append(f.read())\n",
    "\n",
    "\n",
    "    documentation_text=\"/n\".join(all_text)\n",
    "    text_splitter =  CharacterTextSplitter(\n",
    "        separator='\\n',\n",
    "        chunk_overlap=0,\n",
    "        chunk_size=600\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.create_documents([documentation_text])\n",
    "    texts=[doc.page_content for doc in texts]\n",
    "\n",
    "    embedding = OpenAIEmbeddings()\n",
    "\n",
    "    vector_store=Chroma.from_texts(\n",
    "      texts=texts,\n",
    "      embedding=embedding,\n",
    "      persist_directory=r'C:\\Users\\Naveen Reddy\\Downloads\\chatbot', \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "df774f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_chat_bot():\n",
    "    \n",
    "    chat_history=[]\n",
    "\n",
    "    vectordb = Chroma(persist_directory=r'C:\\Users\\Naveen Reddy\\Downloads\\chatbot', embedding_function=embedding)\n",
    "\n",
    "    template = \"\"\"Given the following chat history and a follow up question, rephrase the follow up input question to be a standalone question.\n",
    "    Or end the conversation if it seems like it's done.\n",
    "    Chat History:\\\"\"\"\n",
    "    {chat_history}\n",
    "    \\\"\"\"\n",
    "    Follow Up Input: \\\"\"\"\n",
    "    {question}\n",
    "    \\\"\"\"\n",
    "    Standalone question:\"\"\"\n",
    "\n",
    "    condense_question_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "    # [Prompt]\n",
    "    prompt_template = \"\"\"You are a Bot assistant answering any questions about support queries for hyperverge company.\n",
    "    You are given a question and a set of documents.\n",
    "    If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "    If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "    Use bullet points if you have to make a list, only if necessary.\n",
    "    It's ok if you don't know the answer.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    DOCUMENTS:\n",
    "    =========\n",
    "    {context}\n",
    "    =========\n",
    "    Finish by proposing your help for anything else.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    qa_prompt= PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    llm = OpenAI(temperature=0.1)\n",
    "\n",
    "    streaming_llm = OpenAI(\n",
    "        streaming=True,\n",
    "        callback_manager=CallbackManager([\n",
    "            StreamingStdOutCallbackHandler()\n",
    "        ]),\n",
    "        verbose=True,\n",
    "        max_tokens=250,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=condense_question_prompt\n",
    "    )\n",
    "\n",
    "    doc_chain = load_qa_chain(\n",
    "    llm=streaming_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=qa_prompt\n",
    "    )\n",
    "\n",
    "    chatbot = ConversationalRetrievalChain(\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    "    )\n",
    "\n",
    "\n",
    "    question = input(\"Hi! What are you looking for today?\")\n",
    "    chat_history\n",
    "    while True:\n",
    "        result = chatbot(\n",
    "            {\"question\": question, \"chat_history\": chat_history}\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        chat_history.append((result[\"question\"], result[\"answer\"]))\n",
    "        if keyboard.is_pressed('q'):  \n",
    "            break\n",
    "\n",
    "        question = input()\n",
    "        if question.lower() in ['exit', 'quit']:\n",
    "            print(\"Thank you for contacting support...\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "57eb9957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: C:\\Users\\Naveen Reddy\\Downloads\\chatbot\n"
     ]
    }
   ],
   "source": [
    "folder_path=r'C:\\Users\\Naveen Reddy\\Downloads\\chatbot\\Documentation\\Documentation'\n",
    "get_vector_store(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: C:\\Users\\Naveen Reddy\\Downloads\\chatbot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! What are you looking for today?Does digilocker has a retry mechanism\n",
      "\n",
      "Based on the provided documents, it appears that Digilocker has a retry mechanism. The documents show that when an error occurs, the API returns an error code such as \"ER_DIGILOCKER_REPO_SERVICE_CONFIGERROR\" and a message that states \"Error Connecting to digilocker. Please try again after sometime.\" This indicates that the API will attempt to retry the connection to Digilocker if an error occurs. Additionally, the documents also show that the HealthCheck APIs are aimed at monitoring the availability status of Digilocker APIs, and the accountStatus API can be used to check if a mobile and Aadhaar number are registered with Digilocker.\n",
      "\n",
      "I hope this answers your question. Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_chat_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b1a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
